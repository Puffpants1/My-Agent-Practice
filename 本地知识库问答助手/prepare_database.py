import argparse
import os
import shutil
from uuid import uuid4
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from get_embedding_function import get_embedding_function
from langchain_community.vectorstores.chroma import Chroma
from langchain_ollama import OllamaEmbeddings

CHROMA_PATH = './chroma'
DATA_PATH = './data'

def load_documents():
    '''
    ä»æŒ‡å®šç›®å½•åŠ è½½PDFæ–‡æ¡£ï¼Œæ¯ä¸ªé¡µé¢ä½œä¸ºä¸€ä¸ªæ–‡æ¡£
    '''
    loader = PyPDFDirectoryLoader(DATA_PATH)
    documents = loader.load()
    print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£é¡µé¢")
    return documents

def split_documents(documents):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, # æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
        chunk_overlap=200, # å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
    )
    chunks = text_splitter.split_documents(documents)
    print(f"âœ… æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå¾—åˆ° {len(chunks)} ä¸ªæ–‡æœ¬å—")

    # ä¸ºæ¯ä¸ªæ–‡æœ¬å—æ·»åŠ å”¯ä¸€idå¹¶åˆå¹¶å…ƒæ•°æ®
    for chunk in chunks:
        source = chunk.metadata.get('source', 'unknown')
        page = chunk.metadata.get('page', 0)
        unique_id = f'{os.path.basename(source)}_page{page}_{str(uuid4())[:8]}'
        chunk.metadata['id'] = unique_id
        metadata_text = f"æ¥æºï¼š{os.path.basename(source)}ï¼Œé¡µç ï¼š{page}\n"
        chunk.page_content = metadata_text + chunk.page_content

    # ğŸ” æ£€æŸ¥å‰5ä¸ªå—çš„å†…å®¹
    for i, chunk in enumerate(chunks[:5]):
        preview = chunk.page_content.strip().replace("\n", "\\n")
        print(f"ğŸ“„ Chunk {i} é¢„è§ˆ: {preview[:100]}")

    return chunks
    

def clear_database():
    '''
    æ¸…ç©ºæ•°æ®åº“, ç”¨äºå…¨é‡æ›´æ–°
    '''
    if os.path.exists(CHROMA_PATH):
        shutil.rmtree(CHROMA_PATH)
        print("ğŸ—‘ æ•°æ®åº“å·²æ¸…ç©º")


def main():
    clear_database()

    documents = load_documents()
    chunks = split_documents(documents)
    embeddings = get_embedding_function()
    # embeddings = OllamaEmbeddings(model='bge-m3:latest')

    # ğŸ” æ£€æŸ¥åµŒå…¥æ˜¯å¦æ­£å¸¸
    try:
        test_vec = embeddings.embed_query("è¿™æ˜¯ä¸€ä¸ªåµŒå…¥æµ‹è¯•")
        print(f"ğŸ” åµŒå…¥æµ‹è¯•ï¼šè¿”å›å‘é‡ç»´åº¦ {len(test_vec)}, å‰5ä¸ªå€¼ {test_vec[:5]}")
    except Exception as e:
        print(f"âŒ åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
        return
    
    try:
        test_vecs = embeddings.embed_documents([chunk.page_content for chunk in chunks[:3]])
        print(f"ğŸ” æ‰¹é‡åµŒå…¥æµ‹è¯•ï¼Œè¿”å›å‘é‡æ•°ï¼š{len(test_vecs)}ï¼Œå‘é‡ç»´åº¦ï¼š{len(test_vecs[0])}")
    except Exception as e:
        print(f"âŒ æ‰¹é‡åµŒå…¥å¤±è´¥: {e}")
        return

    # ğŸ” æ£€æŸ¥å‰3ä¸ªchunkçš„åµŒå…¥
    for i, chunk in enumerate(chunks[:3]):
        try:
            vec = embeddings.embed_query(chunk.page_content)
            print(f"ğŸ“Œ Chunk {i} åµŒå…¥ç»´åº¦: {len(vec)}")
        except Exception as e:
            print(f"âŒ Chunk {i} åµŒå…¥å¤±è´¥: {e}")

    # âœ… æ­£å¼å†™å…¥ Chroma
    db = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=CHROMA_PATH
    )
    db.persist()
    print(f"ğŸ¯ æˆåŠŸå°† {len(chunks)} ä¸ªæ–‡æœ¬å—æ·»åŠ åˆ°å‘é‡æ•°æ®åº“")

if __name__ == '__main__':
    main()
